{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from transformer_lens.components import TransformerBlock\n",
    "import ridge_utils.npp\n",
    "from ridge_utils.util import make_delayed\n",
    "from ridge_utils.dsutils import make_word_ds\n",
    "from ridge_utils.DataSequence import DataSequence\n",
    "import warnings\n",
    "import pickle\n",
    "from configs import engram_dir\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "from transformer_lens import HookedTransformer\n",
    "from sae_lens import SAE\n",
    "import torch\n",
    "\n",
    "from transformer_lens.utils import tokenize_and_concatenate\n",
    "from huggingface_hub import login\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "from sae_vis.data_config_classes import SaeVisConfig, SaeVisLayoutConfig\n",
    "from sae_vis.data_storing_fns import SaeVisData\n",
    "from configs import huggingface_token\n",
    "\n",
    "login(token=huggingface_token)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "box_dir = os.path.join(engram_dir, 'huth_box/')\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "grids = joblib.load(os.path.join(box_dir, \"grids_huge.jbl\")) # Load TextGrids containing story annotations\n",
    "trfiles = joblib.load(os.path.join(box_dir, \"trfiles_huge.jbl\")) # Load TRFiles containing TR information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded text data\n"
     ]
    }
   ],
   "source": [
    "wordseqs = make_word_ds(grids, trfiles)\n",
    "for story in wordseqs.keys():\n",
    "    wordseqs[story].data = [i.strip() for i in wordseqs[story].data]\n",
    "print(\"Loaded text data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore Participant Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded participant responses\n"
     ]
    }
   ],
   "source": [
    "response_path = os.path.join(box_dir, 'responses', 'full_responses', 'UTS03_responses.jbl')\n",
    "resp_dict = joblib.load(response_path)\n",
    "to_pop = [x for x in resp_dict.keys() if 'canplanetearthfeedtenbillionpeoplepart' in x]\n",
    "for story in to_pop:\n",
    "    del resp_dict[story]\n",
    "train_stories = list(resp_dict.keys())\n",
    "train_stories = [t for t in train_stories if t != \"wheretheressmoke\"]\n",
    "test_stories = [\"wheretheressmoke\"]\n",
    "print(\"Loaded participant responses\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You tried to specify center_unembed=True for a model using logit softcap, but this can't be done! Softcapping is not invariant upon adding a constant Setting center_unembed=False instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b99f8a40938a4d6c886598e4e7342dcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gemma-2-2b into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "from sae_lens import HookedSAETransformer\n",
    "\n",
    "model = HookedSAETransformer.from_pretrained(\"gemma-2-2b\", device=device)\n",
    "#model = HookedTransformer.from_pretrained(\"gemma-2-2b\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = HookedTransformer.from_pretrained(\"gemma-2-2b\", device=device)\n",
    "tokenizer = model.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def override_to_local_attn(model, window_size=512):\n",
    "    for b in model.blocks:  # Possibly a cleaner way by correctly using 'use_local_attn'\n",
    "        if isinstance(b, TransformerBlock):\n",
    "            n_ctx = b.attn.cfg.n_ctx\n",
    "            attn_mask = torch.zeros((n_ctx, n_ctx)).bool()\n",
    "            for i in range(n_ctx):\n",
    "                start_idx = max(0, i-window_size)\n",
    "                attn_mask[i, start_idx:i+1] = True\n",
    "            b.attn.mask = attn_mask.to(device)\n",
    "\n",
    "override_to_local_attn(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_word_boundaries(text_data, tokenizer):\n",
    "    full_story = \" \".join(text_data).strip()\n",
    "    tokenized_story = tokenizer(full_story)['input_ids']\n",
    "\n",
    "    word_boundaries = []  # In the tokenized story\n",
    "    curr_word_idx = 0\n",
    "    curr_word = text_data[curr_word_idx]\n",
    "    curr_token_set = []\n",
    "\n",
    "    if curr_word == '':\n",
    "        curr_word_idx += 1\n",
    "        curr_word = text_data[curr_word_idx]\n",
    "        word_boundaries.append(1)\n",
    "\n",
    "    for token_idx, token in enumerate(tokenized_story):\n",
    "        curr_token_set.append(token)\n",
    "        detokenized_chunk = tokenizer.decode(curr_token_set)\n",
    "        if curr_word in detokenized_chunk:\n",
    "            word_boundaries.append(token_idx)\n",
    "            curr_word_idx += 1\n",
    "            if curr_word_idx == len(text_data):\n",
    "                break\n",
    "            curr_word = text_data[curr_word_idx]\n",
    "            curr_token_set = []\n",
    "\n",
    "            if curr_word == '':  # Edge case\n",
    "                word_boundaries.append(token_idx)\n",
    "                curr_word_idx += 1\n",
    "                if curr_word_idx == len(text_data):\n",
    "                    break\n",
    "                curr_word = text_data[curr_word_idx]\n",
    "\n",
    "    return tokenized_story, word_boundaries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load features of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_layer = 12\n",
    "brain_region = 'ac'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'pickles/selected_features/direct_regression_L{model_layer}_{brain_region}.pkl', 'rb') as f:\n",
    "    results = pickle.load(f)\n",
    "top_indices = results['top_indices']\n",
    "descriptions = results['descriptions']\n",
    "mean_loading = results['mean_loading']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "release = \"gemma-scope-2b-pt-res-canonical\"\n",
    "sae_id = f\"layer_{model_layer}/width_16k/canonical\"\n",
    "sae = SAE.from_pretrained(release, sae_id)[0].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = []\n",
    "for train_story in train_stories:\n",
    "    ws = wordseqs[train_story]\n",
    "    text_data = ws.data\n",
    "    tokenized_story, word_boundaries = find_word_boundaries(text_data, tokenizer)\n",
    "    # Break into chunks of size 787\n",
    "    for start_idx in range(0, len(tokenized_story), 787):\n",
    "        chunk = tokenized_story[start_idx:start_idx + 787]\n",
    "        # If chunk is too small, take an overlapping chunk from the end\n",
    "        if len(chunk) < 787:\n",
    "            chunk = tokenized_story[-787:]\n",
    "        all_tokens.append(torch.tensor(chunk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = torch.vstack(all_tokens).T.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([787, 301])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<bos>so i'm standing in the phone box and i put twenty pence in and i phone my boyfriend and he answers so i put a pound coin in and just as we're about to speak i suddenly feel the door behind me open a hand appears out of nowhere and i watch it press down the switchhook of the phone i hear the pound coin drop to the bottom and i watch this hand remove the coin earlier that evening i had actually been staying at my boyfriend's flat he lived on main university campus in birmingham city center he was away on work experience and he'd given me the keys to his flat which was great for me because i lived off main campus a another site in handsworth word a short bus ride away but after spending a few nights there and a really long day in the library i suddenly had this urge that i needed to go home i just needed to be in my own space i needed fresh clothes i wanted to sleep in my own bed and i really missed my flatmates but as soon as i thought it a voice in my head said don't go but i want to go and again the voice said don't go now to tell you the truth i wasn't really sure what to make of this voice it made me quite anxious but i couldn't quite figure out what it meant or what could possibly happen and i had to think about it for a while and eventually i decided no i'm gonna go home but to appease this voice in my head i decided to remove my jewelry my watch leave my cards behind and my cash but just take enough money for my bus fare there and to return the following morning and enough also to phone my boyfriend from the pay phone on campus and so i set off and it was probably about i don't know eight thirty in the evening so i was studying in birmingham and it their subways particularly when i was there had a reputation for being quite seedy and unsafe and i had actually been flashed at a number of times down there but on this particular occasion as i walked through the tunnel i noticed a female police officer ahead of me and i remember thinking this was a good omen i kind of broke my stride so we were almost walking in tandem without making it too obvious and then i exit the subway and i get to my bus stop and usually regardless of what the timetable says i always have to wait half an hour for my bus but on this particular evening it comes within minutes so i when i get on the bus i actually sit near the bus driver because i think this is the most sensible thing to do and then i arrive at my stop and at this point i'd say i'm pretty much on the home stretch all i have to do now is get home and as i'm walking along this street i suddenly notice an empty phone box on the corner and in that moment i decide i'm gonna call my boyfriend from there now i make this decision because the pay phone on campus serves about fifty people and it's always ringing off the hook so if i really wanna speak to my boyfriend tonight this is my best chance i realize at this point i'm slightly deviating from the plan of just going straight home but i can distinctly remember as i felt the door close behind me i'm safe in here it's a box so i put my money in and i call my boyfriend and then suddenly i feel the door behind me open a hand appears out of nowhere and i watch it press down the switchhook of the phone i hear the pound coin drop to the bottom and i watch this hand remove the coin i don't remember dropping the receiver but i do remember seeing it swing from side to side suddenly these arms tighten around me and i can feel something sharp against my neck and i'm scared i'm so scared my mouth is dry and i can hardly breathe and my heart is beating so fast i\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(all_tokens[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18f35cb6f68046be95608f103b1218a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Forward passes to cache data for vis:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90be943e891248d4b760ab7570e80e9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting vis data from cached data:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Task                                           </span>┃<span style=\"font-weight: bold\"> Time   </span>┃<span style=\"font-weight: bold\"> Pct % </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
       "│ (1) Forward passes to gather model activations │ 26.41s │ 78.5% │\n",
       "│ (2) Getting data for sequences                 │ 5.43s  │ 16.1% │\n",
       "│ (3) Getting data for non-sequence components   │ 0.42s  │ 1.2%  │\n",
       "│ (?) Unaccounted time                           │ 1.38s  │ 4.1%  │\n",
       "└────────────────────────────────────────────────┴────────┴───────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mTask                                          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mTime  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mPct %\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
       "│ (1) Forward passes to gather model activations │ 26.41s │ 78.5% │\n",
       "│ (2) Getting data for sequences                 │ 5.43s  │ 16.1% │\n",
       "│ (3) Getting data for non-sequence components   │ 0.42s  │ 1.2%  │\n",
       "│ (?) Unaccounted time                           │ 1.38s  │ 4.1%  │\n",
       "└────────────────────────────────────────────────┴────────┴───────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sae_vis_data = SaeVisData.create(\n",
    "    sae=sae,\n",
    "    model=model,\n",
    "    tokens=all_tokens.T,  # 8192\n",
    "    cfg=SaeVisConfig(features=top_indices),  # 256\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "sae_vis_data.save_feature_centric_vis(filename=\"demo_feature_vis.html\", feature=int(top_indices[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
