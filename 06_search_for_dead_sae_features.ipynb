{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from transformer_lens.components import TransformerBlock\n",
    "import ridge_utils.npp\n",
    "from ridge_utils.util import make_delayed\n",
    "from ridge_utils.dsutils import make_word_ds\n",
    "from ridge_utils.DataSequence import DataSequence\n",
    "import warnings\n",
    "from configs import engram_dir\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "from transformer_lens import HookedTransformer\n",
    "from sae_lens import SAE\n",
    "import torch\n",
    "\n",
    "from transformer_lens.utils import tokenize_and_concatenate\n",
    "from huggingface_hub import login\n",
    "from configs import huggingface_token\n",
    "\n",
    "login(token=huggingface_token)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "box_dir = os.path.join(engram_dir, 'huth_box/')\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "# Some parameters\n",
    "trim_start = 50 # Trim 50 TRs off the start of the story\n",
    "trim_end = 5 # Trim 5 off the back\n",
    "ndelays = 4 # We use 4 FIR delays (2 seconds, 4 seconds, 6 seconds, 8 seconds)\n",
    "delays = range(1, ndelays + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "grids = joblib.load(os.path.join(box_dir, \"grids_huge.jbl\")) # Load TextGrids containing story annotations\n",
    "trfiles = joblib.load(os.path.join(box_dir, \"trfiles_huge.jbl\")) # Load TRFiles containing TR information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded text data\n"
     ]
    }
   ],
   "source": [
    "wordseqs = make_word_ds(grids, trfiles)\n",
    "for story in wordseqs.keys():\n",
    "    wordseqs[story].data = [i.strip() for i in wordseqs[story].data]\n",
    "print(\"Loaded text data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore Participant Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded participant responses\n"
     ]
    }
   ],
   "source": [
    "response_path = os.path.join(box_dir, 'responses', 'full_responses', 'UTS03_responses.jbl')\n",
    "resp_dict = joblib.load(response_path)\n",
    "to_pop = [x for x in resp_dict.keys() if 'canplanetearthfeedtenbillionpeoplepart' in x]\n",
    "for story in to_pop:\n",
    "    del resp_dict[story]\n",
    "train_stories = list(resp_dict.keys())\n",
    "train_stories = [t for t in train_stories if t != \"wheretheressmoke\"]\n",
    "test_stories = [\"wheretheressmoke\"]\n",
    "print(\"Loaded participant responses\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You tried to specify center_unembed=True for a model using logit softcap, but this can't be done! Softcapping is not invariant upon adding a constant Setting center_unembed=False instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d962144bf984a8aa11dcbcf14507105",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gemma-2-2b into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\"gemma-2-2b\", device=device)\n",
    "tokenizer = model.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def override_to_local_attn(model, window_size=512):\n",
    "    for b in model.blocks:  # Possibly a cleaner way by correctly using 'use_local_attn'\n",
    "        if isinstance(b, TransformerBlock):\n",
    "            n_ctx = b.attn.cfg.n_ctx\n",
    "            attn_mask = torch.zeros((n_ctx, n_ctx)).bool()\n",
    "            for i in range(n_ctx):\n",
    "                start_idx = max(0, i-window_size)\n",
    "                attn_mask[i, start_idx:i+1] = True\n",
    "            b.attn.mask = attn_mask.to(device)\n",
    "\n",
    "override_to_local_attn(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_word_boundaries(text_data, tokenizer):\n",
    "    full_story = \" \".join(text_data).strip()\n",
    "    tokenized_story = tokenizer(full_story)['input_ids']\n",
    "\n",
    "    word_boundaries = []  # In the tokenized story\n",
    "    curr_word_idx = 0\n",
    "    curr_word = text_data[curr_word_idx]\n",
    "    curr_token_set = []\n",
    "\n",
    "    if curr_word == '':\n",
    "        curr_word_idx += 1\n",
    "        curr_word = text_data[curr_word_idx]\n",
    "        word_boundaries.append(1)\n",
    "\n",
    "    for token_idx, token in enumerate(tokenized_story):\n",
    "        curr_token_set.append(token)\n",
    "        detokenized_chunk = tokenizer.decode(curr_token_set)\n",
    "        if curr_word in detokenized_chunk:\n",
    "            word_boundaries.append(token_idx)\n",
    "            curr_word_idx += 1\n",
    "            if curr_word_idx == len(text_data):\n",
    "                break\n",
    "            curr_word = text_data[curr_word_idx]\n",
    "            curr_token_set = []\n",
    "\n",
    "            if curr_word == '':  # Edge case\n",
    "                word_boundaries.append(token_idx)\n",
    "                curr_word_idx += 1\n",
    "                if curr_word_idx == len(text_data):\n",
    "                    break\n",
    "                curr_word = text_data[curr_word_idx]\n",
    "\n",
    "    return tokenized_story, word_boundaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dead_units(layer):\n",
    "    release = \"gemma-scope-2b-pt-res-canonical\"\n",
    "    sae_id = f\"layer_{layer}/width_16k/canonical\"\n",
    "    sae = SAE.from_pretrained(release, sae_id)[0].to(device)\n",
    "\n",
    "    all_features = None #np.zeros(16384)\n",
    "    for train_story in train_stories:\n",
    "        ws = wordseqs[train_story]\n",
    "        text_data = ws.data\n",
    "        tokenized_story, word_boundaries = find_word_boundaries(text_data, tokenizer)\n",
    "        hook_key = f'blocks.{layer}.hook_resid_post'\n",
    "        with torch.no_grad():\n",
    "            _, cache = model.run_with_cache(\n",
    "                torch.tensor(tokenized_story).to(device),\n",
    "                prepend_bos=True,\n",
    "                names_filter=lambda name: name==hook_key,\n",
    "            )\n",
    "        llm_response = cache[hook_key][0, word_boundaries, :]\n",
    "        with torch.no_grad():\n",
    "            feature_acts = sae.encode(llm_response)\n",
    "        n_samples, n_features = feature_acts.shape\n",
    "        dead_features = torch.argwhere(feature_acts.sum(axis=0)==0).cpu().numpy().flatten()\n",
    "        if all_features is None:\n",
    "            all_features = np.zeros(n_features)\n",
    "        all_features[dead_features] += 1\n",
    "    n_stories = len(train_stories)\n",
    "    dead_units = np.argwhere(all_features == n_stories).squeeze()\n",
    "    alive_units = np.argwhere(all_features != n_stories).squeeze()\n",
    "    return dead_units, alive_units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0 has 114 dead units\n",
      "Layer 3 has 782 dead units\n",
      "Layer 6 has 2442 dead units\n",
      "Layer 9 has 2050 dead units\n",
      "Layer 12 has 1630 dead units\n",
      "Layer 15 has 2319 dead units\n",
      "Layer 18 has 2142 dead units\n",
      "Layer 21 has 1884 dead units\n",
      "Layer 24 has 1970 dead units\n"
     ]
    }
   ],
   "source": [
    "for layer in range(0, 25, 3):\n",
    "    dead_units, alive_units = get_dead_units(layer)\n",
    "    print(f\"Layer {layer} has {len(dead_units)} dead units\")\n",
    "    with open(os.path.join(\"pickles\", \"dead_sae_unit_indices\", f\"gemma2b_L{layer}.pkl\"), \"wb\") as f:\n",
    "        results = {\n",
    "            \"dead_units\": dead_units,\n",
    "            \"alive_units\": alive_units\n",
    "        }\n",
    "        pickle.dump(results, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
