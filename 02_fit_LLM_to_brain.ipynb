{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home04/cfang/.conda/envs/sae/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from transformer_lens.components import TransformerBlock\n",
    "import ridge_utils.npp\n",
    "from ridge_utils.util import make_delayed\n",
    "from ridge_utils.dsutils import make_word_ds\n",
    "from ridge_utils.DataSequence import DataSequence\n",
    "import warnings\n",
    "from configs import engram_dir\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "from transformer_lens import HookedTransformer\n",
    "from sae_lens import SAE\n",
    "import torch\n",
    "\n",
    "from transformer_lens.utils import tokenize_and_concatenate\n",
    "from huggingface_hub import login\n",
    "from configs import huggingface_token\n",
    "\n",
    "login(token=huggingface_token)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "box_dir = os.path.join(engram_dir, 'huth_box/')\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "# Some parameters\n",
    "trim_start = 50 # Trim 50 TRs off the start of the story\n",
    "trim_end = 5 # Trim 5 off the back\n",
    "ndelays = 4 # We use 4 FIR delays (2 seconds, 4 seconds, 6 seconds, 8 seconds)\n",
    "delays = range(1, ndelays + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "grids = joblib.load(os.path.join(box_dir, \"grids_huge.jbl\")) # Load TextGrids containing story annotations\n",
    "trfiles = joblib.load(os.path.join(box_dir, \"trfiles_huge.jbl\")) # Load TRFiles containing TR information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded text data\n"
     ]
    }
   ],
   "source": [
    "wordseqs = make_word_ds(grids, trfiles)\n",
    "for story in wordseqs.keys():\n",
    "    wordseqs[story].data = [i.strip() for i in wordseqs[story].data]\n",
    "print(\"Loaded text data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore Participant Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded participant responses\n"
     ]
    }
   ],
   "source": [
    "response_path = os.path.join(box_dir, 'responses', 'full_responses', 'UTS03_responses.jbl')\n",
    "resp_dict = joblib.load(response_path)\n",
    "to_pop = [x for x in resp_dict.keys() if 'canplanetearthfeedtenbillionpeoplepart' in x]\n",
    "for story in to_pop:\n",
    "    del resp_dict[story]\n",
    "train_stories = list(resp_dict.keys())\n",
    "train_stories = [t for t in train_stories if t != \"wheretheressmoke\"]\n",
    "test_stories = [\"wheretheressmoke\"]\n",
    "print(\"Loaded participant responses\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You tried to specify center_unembed=True for a model using logit softcap, but this can't be done! Softcapping is not invariant upon adding a constant Setting center_unembed=False instead.\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:01<00:00,  2.79it/s]\n",
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gemma-2-2b into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\"gemma-2-2b\", device=device)\n",
    "tokenizer = model.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def override_to_local_attn(model, window_size=512):\n",
    "    for b in model.blocks:  # Possibly a cleaner way by correctly using 'use_local_attn'\n",
    "        if isinstance(b, TransformerBlock):\n",
    "            n_ctx = b.attn.cfg.n_ctx\n",
    "            attn_mask = torch.zeros((n_ctx, n_ctx)).bool()\n",
    "            for i in range(n_ctx):\n",
    "                start_idx = max(0, i-window_size)\n",
    "                attn_mask[i, start_idx:i+1] = True\n",
    "            b.attn.mask = attn_mask.to(device)\n",
    "\n",
    "override_to_local_attn(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_word_boundaries(text_data, tokenizer):\n",
    "    full_story = \" \".join(text_data).strip()\n",
    "    tokenized_story = tokenizer(full_story)['input_ids']\n",
    "\n",
    "    word_boundaries = []  # In the tokenized story\n",
    "    curr_word_idx = 0\n",
    "    curr_word = text_data[curr_word_idx]\n",
    "    curr_token_set = []\n",
    "\n",
    "    if curr_word == '':\n",
    "        curr_word_idx += 1\n",
    "        curr_word = text_data[curr_word_idx]\n",
    "        word_boundaries.append(1)\n",
    "\n",
    "    for token_idx, token in enumerate(tokenized_story):\n",
    "        curr_token_set.append(token)\n",
    "        detokenized_chunk = tokenizer.decode(curr_token_set)\n",
    "        if curr_word in detokenized_chunk:\n",
    "            word_boundaries.append(token_idx)\n",
    "            curr_word_idx += 1\n",
    "            if curr_word_idx == len(text_data):\n",
    "                break\n",
    "            curr_word = text_data[curr_word_idx]\n",
    "            curr_token_set = []\n",
    "\n",
    "            if curr_word == '':  # Edge case\n",
    "                word_boundaries.append(token_idx)\n",
    "                curr_word_idx += 1\n",
    "                if curr_word_idx == len(text_data):\n",
    "                    break\n",
    "                curr_word = text_data[curr_word_idx]\n",
    "\n",
    "    return tokenized_story, word_boundaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "release = \"gemma-scope-2b-pt-res-canonical\"\n",
    "sae_id = \"layer_7/width_16k/canonical\"\n",
    "sae = SAE.from_pretrained(release, sae_id)[0].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_sae = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33021, 2304)\n"
     ]
    }
   ],
   "source": [
    "llm_train_responses = []\n",
    "for train_story in train_stories:\n",
    "    ws = wordseqs[train_story]\n",
    "    text_data = ws.data\n",
    "    tokenized_story, word_boundaries = find_word_boundaries(text_data, tokenizer)\n",
    "    with torch.no_grad():\n",
    "        _, cache = model.run_with_cache(\n",
    "            torch.tensor(tokenized_story).to(device),\n",
    "            prepend_bos=True,\n",
    "            names_filter=lambda name: name.endswith('hook_resid_post'),\n",
    "        )\n",
    "    llm_response = cache['blocks.7.hook_resid_post'][0, word_boundaries, :]\n",
    "    if use_sae:\n",
    "        with torch.no_grad():\n",
    "            feature_acts = sae.encode(llm_response)\n",
    "            sae_out = sae.decode(feature_acts)\n",
    "        llm_response = sae_out\n",
    "    llm_data_seq = DataSequence(llm_response.cpu().numpy(), ws.split_inds, ws.data_times, ws.tr_times)\n",
    "    interp_llm_response = llm_data_seq.chunksums('lanczos', window=3)\n",
    "    interp_llm_response = ridge_utils.npp.zs(interp_llm_response[10:-5])\n",
    "    llm_train_responses.append(interp_llm_response)\n",
    "\n",
    "llm_train_responses = np.vstack(llm_train_responses)\n",
    "print(llm_train_responses.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3039, 2304)\n"
     ]
    }
   ],
   "source": [
    "llm_test_responses = []\n",
    "for test_story in test_stories:\n",
    "    ws = wordseqs[test_story]\n",
    "    text_data = ws.data\n",
    "    tokenized_story, word_boundaries = find_word_boundaries(text_data, tokenizer)\n",
    "    with torch.no_grad():\n",
    "        _, cache = model.run_with_cache(\n",
    "            torch.tensor(tokenized_story).to(device),\n",
    "            prepend_bos=True,\n",
    "            names_filter=lambda name: name.startswith('blocks.7.hook_resid_post'),\n",
    "        )\n",
    "    llm_response = cache['blocks.7.hook_resid_post'][0, word_boundaries, :]\n",
    "    if use_sae:\n",
    "        with torch.no_grad():\n",
    "            feature_acts = sae.encode(llm_response)\n",
    "            sae_out = sae.decode(feature_acts)\n",
    "        llm_response = sae_out\n",
    "    llm_data_seq = DataSequence(llm_response.cpu().numpy(), ws.split_inds, ws.data_times, ws.tr_times)\n",
    "    interp_llm_response = llm_data_seq.chunksums('lanczos', window=3)\n",
    "    interp_llm_response = ridge_utils.npp.zs(interp_llm_response[10:-5])\n",
    "    llm_test_responses.append(interp_llm_response[40:])\n",
    "\n",
    "llm_test_responses = np.vstack(llm_test_responses)\n",
    "print(llm_test_responses.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "del cache\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add FIR delays\n",
    "delRstim = make_delayed(llm_train_responses, delays)\n",
    "delPstim = make_delayed(llm_test_responses, delays)\n",
    "\n",
    "alphas = np.logspace(1, 4, 15) # Equally log-spaced ridge parameters between 10 and 10000. \n",
    "nboots = 3 # Number of cross-validation ridge regression runs. You can lower this number to increase speed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rresp = np.vstack([resp_dict[story] for story in train_stories])\n",
    "Presp = np.vstack([resp_dict[story][40:] for story in test_stories])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "selected_features = np.random.choice(Rresp.shape[1], size=5000, replace=False)\n",
    "np.random.seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "X_train = delRstim\n",
    "X_test = delPstim\n",
    "Y_train = Rresp[:, selected_features]\n",
    "Y_test = Presp[:, selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import numpy as np\n",
    "\n",
    "# Define the range of alphas for cross-validation\n",
    "alphas = np.logspace(1, 5, 15)  # Regularization strengths\n",
    "\n",
    "# Create a pipeline with standard scaling and ridge regression\n",
    "ridge_pipeline = make_pipeline(\n",
    "    StandardScaler(),  # Standardize features by removing the mean and scaling to unit variance\n",
    "    Ridge()  # Ridge regression\n",
    ")\n",
    "\n",
    "# Set up the parameter grid for alpha\n",
    "param_grid = {'ridge__alpha': alphas}\n",
    "\n",
    "# Use GridSearchCV to perform cross-validation with parallel processing\n",
    "grid_search = GridSearchCV(\n",
    "    ridge_pipeline, param_grid, cv=3, scoring='neg_mean_squared_error', n_jobs=20\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "grid_search.fit(X_train, Y_train)\n",
    "\n",
    "# Retrieve the best alpha\n",
    "best_alpha = grid_search.best_params_['ridge__alpha']\n",
    "print(f\"Best alpha: {best_alpha}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned regression weights: (100, 9216)\n"
     ]
    }
   ],
   "source": [
    "# After fitting the grid_search\n",
    "best_ridge_model = grid_search.best_estimator_.named_steps['ridge']\n",
    "regression_weights = best_ridge_model.coef_\n",
    "\n",
    "print(\"Learned regression weights:\", regression_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save regression weights to a pickle file\n",
    "with open('gemma_regression_weights.pkl', 'wb') as f:\n",
    "    pickle.dump(regression_weights, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean R value across features: 0.445\n",
      "Mean R^2 value across features: 0.199\n",
      "\n",
      "R value range: [0.395, 0.551]\n",
      "R^2 value range: [0.156, 0.304]\n"
     ]
    }
   ],
   "source": [
    "# Get predictions on test set\n",
    "X_test_scaled = grid_search.best_estimator_.named_steps['standardscaler'].transform(X_test)\n",
    "Y_pred = grid_search.best_estimator_.named_steps['ridge'].predict(X_test_scaled)\n",
    "\n",
    "# Calculate R and R^2 values for each feature\n",
    "r_values = []\n",
    "r2_values = []\n",
    "\n",
    "for i in range(Y_test.shape[1]):\n",
    "    # Calculate correlation coefficient (R)\n",
    "    r = np.corrcoef(Y_test[:,i], Y_pred[:,i])[0,1]\n",
    "    r_values.append(r)\n",
    "    \n",
    "    # Calculate R^2\n",
    "    r2 = r**2\n",
    "    r2_values.append(r2)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "r_values = np.array(r_values)\n",
    "r2_values = np.array(r2_values)\n",
    "\n",
    "print(f\"Mean R value across features: {r_values.mean():.3f}\")\n",
    "print(f\"Mean R^2 value across features: {r2_values.mean():.3f}\")\n",
    "print(f\"\\nR value range: [{r_values.min():.3f}, {r_values.max():.3f}]\")\n",
    "print(f\"R^2 value range: [{r2_values.min():.3f}, {r2_values.max():.3f}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
