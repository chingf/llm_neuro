{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from transformer_lens.components import TransformerBlock\n",
    "import ridge_utils.npp\n",
    "from ridge_utils.util import make_delayed\n",
    "from ridge_utils.dsutils import make_word_ds\n",
    "from ridge_utils.DataSequence import DataSequence\n",
    "import warnings\n",
    "import pickle\n",
    "from configs import engram_dir\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "from transformer_lens import HookedTransformer\n",
    "from sae_lens import SAE\n",
    "import torch\n",
    "\n",
    "from transformer_lens.utils import tokenize_and_concatenate\n",
    "from huggingface_hub import login\n",
    "from configs import huggingface_token\n",
    "\n",
    "login(token=huggingface_token)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "box_dir = os.path.join(engram_dir, 'huth_box/')\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "grids = joblib.load(os.path.join(box_dir, \"grids_huge.jbl\")) # Load TextGrids containing story annotations\n",
    "trfiles = joblib.load(os.path.join(box_dir, \"trfiles_huge.jbl\")) # Load TRFiles containing TR information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded text data\n"
     ]
    }
   ],
   "source": [
    "wordseqs = make_word_ds(grids, trfiles)\n",
    "for story in wordseqs.keys():\n",
    "    wordseqs[story].data = [i.strip() for i in wordseqs[story].data]\n",
    "print(\"Loaded text data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore Participant Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded participant responses\n"
     ]
    }
   ],
   "source": [
    "response_path = os.path.join(box_dir, 'responses', 'full_responses', 'UTS03_responses.jbl')\n",
    "resp_dict = joblib.load(response_path)\n",
    "to_pop = [x for x in resp_dict.keys() if 'canplanetearthfeedtenbillionpeoplepart' in x]\n",
    "for story in to_pop:\n",
    "    del resp_dict[story]\n",
    "train_stories = list(resp_dict.keys())\n",
    "train_stories = [t for t in train_stories if t != \"wheretheressmoke\"]\n",
    "test_stories = [\"wheretheressmoke\"]\n",
    "print(\"Loaded participant responses\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You tried to specify center_unembed=True for a model using logit softcap, but this can't be done! Softcapping is not invariant upon adding a constant Setting center_unembed=False instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3ba8ff1c21d4b4d8a9e53afb5740ec1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gemma-2-2b into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\"gemma-2-2b\", device=device)\n",
    "tokenizer = model.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def override_to_local_attn(model, window_size=512):\n",
    "    for b in model.blocks:  # Possibly a cleaner way by correctly using 'use_local_attn'\n",
    "        if isinstance(b, TransformerBlock):\n",
    "            n_ctx = b.attn.cfg.n_ctx\n",
    "            attn_mask = torch.zeros((n_ctx, n_ctx)).bool()\n",
    "            for i in range(n_ctx):\n",
    "                start_idx = max(0, i-window_size)\n",
    "                attn_mask[i, start_idx:i+1] = True\n",
    "            b.attn.mask = attn_mask.to(device)\n",
    "\n",
    "override_to_local_attn(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_word_boundaries(text_data, tokenizer):\n",
    "    full_story = \" \".join(text_data).strip()\n",
    "    tokenized_story = tokenizer(full_story)['input_ids']\n",
    "\n",
    "    word_boundaries = []  # In the tokenized story\n",
    "    curr_word_idx = 0\n",
    "    curr_word = text_data[curr_word_idx]\n",
    "    curr_token_set = []\n",
    "\n",
    "    if curr_word == '':\n",
    "        curr_word_idx += 1\n",
    "        curr_word = text_data[curr_word_idx]\n",
    "        word_boundaries.append(1)\n",
    "\n",
    "    for token_idx, token in enumerate(tokenized_story):\n",
    "        curr_token_set.append(token)\n",
    "        detokenized_chunk = tokenizer.decode(curr_token_set)\n",
    "        if curr_word in detokenized_chunk:\n",
    "            word_boundaries.append(token_idx)\n",
    "            curr_word_idx += 1\n",
    "            if curr_word_idx == len(text_data):\n",
    "                break\n",
    "            curr_word = text_data[curr_word_idx]\n",
    "            curr_token_set = []\n",
    "\n",
    "            if curr_word == '':  # Edge case\n",
    "                word_boundaries.append(token_idx)\n",
    "                curr_word_idx += 1\n",
    "                if curr_word_idx == len(text_data):\n",
    "                    break\n",
    "                curr_word = text_data[curr_word_idx]\n",
    "\n",
    "    return tokenized_story, word_boundaries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load features of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_layer = 12\n",
    "brain_region = 'broca'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'pickles/selected_features/direct_regression_L{model_layer}_{brain_region}.pkl', 'rb') as f:\n",
    "    results = pickle.load(f)\n",
    "top_indices = results['top_indices']\n",
    "descriptions = results['descriptions']\n",
    "mean_loading = results['mean_loading']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_activating_examples():\n",
    "    release = \"gemma-scope-2b-pt-res-canonical\"\n",
    "    sae_id = f\"layer_{model_layer}/width_16k/canonical\"\n",
    "    sae = SAE.from_pretrained(release, sae_id)[0].to(device)\n",
    "\n",
    "    results = {idx: [] for idx in top_indices}\n",
    "\n",
    "    for train_story in train_stories:\n",
    "        ws = wordseqs[train_story]\n",
    "        text_data = ws.data\n",
    "        tokenized_story, word_boundaries = find_word_boundaries(text_data, tokenizer)\n",
    "        hook_key = f'blocks.{model_layer}.hook_resid_post'\n",
    "        with torch.no_grad():\n",
    "            _, cache = model.run_with_cache(\n",
    "                torch.tensor(tokenized_story).to(device),\n",
    "                prepend_bos=True,\n",
    "                names_filter=lambda name: name==hook_key,\n",
    "            )\n",
    "        llm_response = cache[hook_key][0, word_boundaries, :]\n",
    "        with torch.no_grad():\n",
    "            feature_acts = sae.encode(llm_response).cpu().numpy()\n",
    "        n_samples, n_features = feature_acts.shape\n",
    "        for idx in top_indices:\n",
    "            for sample in range(n_samples):\n",
    "                val = feature_acts[sample, idx]\n",
    "                results[idx].append((train_story, sample, val))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = get_top_activating_examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_results(results):\n",
    "    for i in results.keys():\n",
    "        examples = sorted(results[i], key=lambda x: abs(x[2]), reverse=True)\n",
    "        processed_examples = []\n",
    "        for example in examples:\n",
    "            story, sample, val = example\n",
    "            add_to_processed = True\n",
    "            for prev_examples in processed_examples:\n",
    "                if prev_examples[0] == story:\n",
    "                    if abs(prev_examples[1] - sample) < 5:\n",
    "                        add_to_processed = False\n",
    "                        break\n",
    "            if add_to_processed:\n",
    "                processed_examples.append(example)\n",
    "        results[i] = processed_examples\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = process_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1033\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'scientific terms and measurements related to biological and chemical studies'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = top_indices[4]\n",
    "print(i)\n",
    "descriptions[np.argmax(top_indices==i)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " on it called miss connections and that's what funny\n",
      " moment and we're like you know hitting each other\n",
      " forgave me and told me she loved me and felt\n",
      " a second if i'm gonna bob my head in\n",
      " but i do remember seeing it swing from side to side\n",
      " kind of a milkshake ice cream sandwich which he's\n",
      "re coming in about a ten twelve degree angle he'\n",
      " she's googling me and i don't know\n",
      " almost died i almost died and i am so glad we\n",
      " is now setting so i'm thinking right we got\n",
      " by four across the prairie just like you know bouncing across\n",
      " homosexuality thing and i was completely thrown and thought that maybe\n",
      "m comforted to to discover that london exa is exactly\n",
      " the guy goes no no no so my mother and my\n",
      "m saying you can't have them and i won\n",
      " the greatest website ever so i started doing this bit all\n"
     ]
    }
   ],
   "source": [
    "printed_examples = 0\n",
    "skipped_examples = 0\n",
    "vals = []\n",
    "for example_idx, example in enumerate(results[i]):\n",
    "    train_story, sample, val = example\n",
    "    if sample < 10:\n",
    "        skipped_examples += 1\n",
    "        continue\n",
    "    ws = wordseqs[train_story]\n",
    "    text_data = ws.data\n",
    "    tokenized_story, word_boundaries = find_word_boundaries(text_data, tokenizer)\n",
    "    start_idx = max(0, sample-10)\n",
    "    print(tokenizer.decode(tokenized_story[start_idx:sample+1]))\n",
    "    printed_examples += 1\n",
    "    vals.append(val)\n",
    "    if printed_examples > 15:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93\n"
     ]
    }
   ],
   "source": [
    "print(skipped_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14.985634, 14.227369, 13.567108, 12.950641, 11.625395, 11.555357, 10.855405, 9.583924, 9.558023, 9.501002, 9.378521, 9.339999, 9.28685, 8.784471, 8.752684, 8.576482]\n"
     ]
    }
   ],
   "source": [
    "print(vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
